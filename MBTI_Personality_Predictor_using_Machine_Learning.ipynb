{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBTI Personality Predictor using Machine Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L33ZOzQFa4FN",
        "B69SdBntu15m",
        "Q2o-m1duPej8",
        "SlNj-xgMkZHu",
        "z0kxDU0Ckm5X",
        "R9T8J8rG2UUR",
        "jkHvOZl7lCqU",
        "XoYn1XkQlVAl",
        "BT0oDWJglZL9",
        "zrR2_SbDlexo",
        "Yh2CqoSGltLP",
        "j_yrx5nSl4JL",
        "6FRp8jkvo093",
        "PBaFuiqUourU",
        "bRHmuUXQcpQm",
        "pdPTUgjVHDvf",
        "_1JRGyq6HelK",
        "9CI1aqweH8Qo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpe6QdAcp1Co"
      },
      "source": [
        "##**Trabajo de Fin de Máster**\n",
        "Salomé A. Sepúlveda Fontaine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P024zYXXnvpM"
      },
      "source": [
        "##**Preparación del entorno**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nHVq7F4tTUk"
      },
      "source": [
        "####Importación de Librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFLBrC5oI7SY"
      },
      "source": [
        "# Data Analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from numpy import savetxt\n",
        "from numpy import loadtxt\n",
        "import pickle as pkl\n",
        "from scipy import sparse\n",
        "\n",
        "# Data Visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import wordcloud\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Text Processing\n",
        "import re\n",
        "import itertools\n",
        "import string\n",
        "import collections\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Machine Learning packages\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import sklearn.cluster as cluster\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Model training and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "#Metrics\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# Ignore noise warning\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofQTZj9XhHRw"
      },
      "source": [
        "####Funciones predefinidas, explícitas y implícitas.\n",
        "Análisis, visualizacion y procesamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2ROzsmEfo3W"
      },
      "source": [
        "# Se crea diccionario con las iniciales de cada tipo de personalidad\n",
        "mbti = {'I':'Introversion', 'E':'Extroversion', 'N':'Intuition', \n",
        "        'S':'Sensing', 'T':'Thinking', 'F': 'Feeling', \n",
        "        'J':'Judging', 'P': 'Perceiving'}\n",
        "def var_row(row):\n",
        "    l = []\n",
        "    for i in row.split('|||'):\n",
        "        l.append(len(i.split()))\n",
        "    return np.var(l)\n",
        "    \n",
        "def unique_words(a, b, numWords):\n",
        "  ## Palabras únicas tipo 'a'\n",
        "  words_a = (XX[X[a] == 1].mean() / XX[X[b] == 1].mean()).sort_values().rename(lambda x: x[2:]).tail(numWords)\n",
        "  print('* Unique ' + mbti[a] + ' Words:')\n",
        "  print(words_a)\n",
        "  ## Palabras únicas tipo 'b'\n",
        "  words_b = (XX[X[b] == 1].mean() / XX[X[a] == 1].mean()).sort_values().rename(lambda x: x[2:]).tail(numWords)\n",
        "  print('\\n* Unique ' + mbti[b] + ' Words:')\n",
        "  print(words_b)\n",
        "\n",
        "  ## Representación en gráfico de barras\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.subplot(131)\n",
        "  words_a.plot.barh()\n",
        "  plt.title(mbti[a] + ' vs ' + mbti[b])\n",
        "  plt.subplot(133)\n",
        "  words_b.plot.barh()\n",
        "  plt.title(mbti[b] + ' vs ' + mbti[a])\n",
        "  plt.show()\n",
        "\n",
        "def new_Words(a, b, numWords):\n",
        "  ## Palabras únicas tipo 'a'\n",
        "  words_a = (testX[X[a] == 1].mean() / testX[X[b] == 1].mean()).sort_values().rename(lambda x: x[2:]).tail(numWords)\n",
        "  print('* Unique ' + mbti[a] + ' Words:')\n",
        "  print(words_a)\n",
        "  ## Palabras únicas tipo 'b'\n",
        "  words_b = (testX[X[b] == 1].mean() / testX[X[a] == 1].mean()).sort_values().rename(lambda x: x[2:]).tail(numWords)\n",
        "  print('\\n* Unique ' + mbti[b] + ' Words:')\n",
        "  print(words_b)\n",
        "\n",
        "  ## Representación en gráfico de barras\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.subplot(131)\n",
        "  words_a.plot.barh()\n",
        "  plt.title(mbti[a] + ' vs ' + mbti[b])\n",
        "  plt.subplot(133)\n",
        "  words_b.plot.barh()\n",
        "  plt.title(mbti[b] + ' vs ' + mbti[a])\n",
        "  plt.show()\n",
        "\n",
        "def preprocess_text(df, remove_special=True):\n",
        "    texts = df['posts'].copy()\n",
        "    labels = df['type'].copy()\n",
        "\n",
        "    #Remove links \n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'https?:\\/\\/.*?[\\s+]', '', x.replace(\"|\",\" \") + \" \"))\n",
        "    \n",
        "    #Keep the End Of Sentence characters\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', x + \" \"))\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', x + \" \"))\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', x + \" \"))\n",
        "    \n",
        "    #Strip Punctation\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[\\.+]', \".\",x))\n",
        "\n",
        "    #Remove multiple fullstops\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
        "\n",
        "    #Remove Non-words\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','',x))\n",
        "\n",
        "    #Convert posts to lowercase\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: x.lower())\n",
        "\n",
        "    #Remove multiple letter repeating words\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','',x)) \n",
        "\n",
        "    #Remove very short or long words\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{0,3})?\\b','',x)) \n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{30,1000})?\\b','',x))\n",
        "\n",
        "    #Remove MBTI Personality Words - crutial in order to get valid model accuracy estimation for unseen data. \n",
        "    if remove_special:\n",
        "        pers_types = ['INFP' ,'INFJ', 'INTP', 'INTJ', 'ENTP', 'ENFP', 'ISTP' ,'ISFP' ,'ENTJ', 'ISTJ','ENFJ', 'ISFJ' ,'ESTP', 'ESFP' ,'ESFJ' ,'ESTJ']\n",
        "        pers_types = [p.lower() for p in pers_types]\n",
        "        p = re.compile(\"(\" + \"|\".join(pers_types) + \")\")\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI-ZM5MMuGt9"
      },
      "source": [
        "Loading Dataset\n",
        "\n",
        "To upload the dataset you can use Google Drive with the code below or download the file [mbti_1.csv.zip](https://github.com/ApusDT/MBTI-Personality-Predictor-using-Machine-Learning/blob/main/mbti_1.csv.zip) and unzip it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmhQEMPXJEDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c97b546-fcdc-417d-9e62-62f08dd88e76"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df=pd.read_csv(\"path/to/file\", encoding='utf-8')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L33ZOzQFa4FN"
      },
      "source": [
        "##**Analisis, descripción y visualización de datos.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOB0iR6hg11s"
      },
      "source": [
        "Visualización del DataFrame.\n",
        "Observamos las 4 primeras filas del conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyGoK45DWwtB"
      },
      "source": [
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlEotxawYPP1"
      },
      "source": [
        "A modo de ejmplo se visualiz la información de una columna. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XQ2xdfWW6N0"
      },
      "source": [
        "df.loc[1,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o_79QZnYWq_"
      },
      "source": [
        "Visualización de post.\n",
        "\n",
        "Como los posts son muchos y no se aprecian en las celdas anteriores, se imprimen en pantalla todos los posts de una fila. Notar que cada post está seprado por \"**|||**\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRFzAARVXwSq"
      },
      "source": [
        "df.loc[1,'posts']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYgKB86EYDBm"
      },
      "source": [
        "Descripción de cantidad y tipo de columnas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd_qiwi8a2Aa"
      },
      "source": [
        "print(df.columns)\n",
        "print(\"Dataframe con {} filas y {} columnas\".format(df.shape[0],df.shape[1]))\n",
        "print(\"Las columnas son: {} y {}\".format(df.columns[0],df.columns[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5BxfC_ag8ob"
      },
      "source": [
        "Búsqueda de valores nulos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4FDjrZ5a727"
      },
      "source": [
        "print(df.info())\n",
        "#contenio de nulls\n",
        "print(\"\\nNumero de valores nulos\")\n",
        "print(df.isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "019mPirE3s5b"
      },
      "source": [
        "Con el método `describe`  se obtiene una visión global de frecuencias y tipos.\n",
        "  \n",
        "\n",
        "*   Hay 16 tipos de indicadores de personalidad únicas, con 1832 ocurrencias.\n",
        "*   No se observan posts repetidos.\n",
        "\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sleXBXMLdQJC"
      },
      "source": [
        "df.describe(include=['object'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL7iHXiF38m7"
      },
      "source": [
        "Se buscan los valores únicos de las columnas del tipo de personalidad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1pqD-fjdWc7"
      },
      "source": [
        "types = np.unique(np.array(df['type']))\n",
        "types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWUb9McO4hc_"
      },
      "source": [
        "Se usará el método `groupby` para separar los distintos grupos de personalidad y hacer un análisis más exhaustivo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcloTBO1dhIP"
      },
      "source": [
        "total = df.groupby(['type']).count().sort_values(by=['posts'],ascending=False)\n",
        "total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPWUkNhA5-MX"
      },
      "source": [
        "**Visualización del número de posts por cada tipo de personalidad.**\n",
        "\n",
        "Se observa que los datos están desbalanceados en el sentido que no existe una distribución equitativa para el número de post por personalidad, como puede apreciarse en el siguiente gráfico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GpmEZeOeiz6"
      },
      "source": [
        "plt.figure(figsize = (12,4))\n",
        "plot = plt.bar(np.array(total.index), height = total['posts'])\n",
        "plot[0].set_color('y')\n",
        "plot[-1].set_color('r')\n",
        "plt.xlabel('Personality types', size = 14)\n",
        "plt.ylabel('No. of posts available', size = 14)\n",
        "plt.title('Total de posts por cada tipo de personalidad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RficWLk59uI0"
      },
      "source": [
        "**SWARM PLOT** :\n",
        "\n",
        "Swarm Plots, también llamados beeswarm plots, grafican todos los puntos de la data para grupo de personalidad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8mV1NX0fNe0"
      },
      "source": [
        "df['words_per_comment'] = df['posts'].apply(lambda x: len(x.split())/50)\n",
        "df['variance_of_word_counts'] = df['posts'].apply(lambda x: var_row(x))\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.swarmplot(\"type\", \"words_per_comment\", data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHN6a4aP-EIe"
      },
      "source": [
        "**Longitud de publicaciones.**\n",
        "\n",
        "**DISTANCE PLOT:**\n",
        "\n",
        "Con seaborn, usando este método, se observa el histograma de la distribución de los datos, por cada columna.\n",
        "*   Se observa que la mayoría de los posts tienen una longitud de entre 7000 y 9000 palabras.\n",
        "*   La línea observada representa el núcleo de la densidad de estimación. \n",
        "*   Es un problema fundamental de suavizado de datos donde se realizan inferencias sobre la población, basadas en una muestra de datos finitos. Esta estimación de la densidad del núcleo es una función definida como la suma de una función del núcleo en cada punto de datos.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM-s34ZDf4UT"
      },
      "source": [
        "df[\"length_posts\"] = df[\"posts\"].apply(len)\n",
        "sns.distplot(df[\"length_posts\"]).set_title(\"Distribución del largo de todos los 50 posts\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxS5mIcr-9RK"
      },
      "source": [
        "**Usuarios y posts**\n",
        "\n",
        "Con la función `extract` se cuenta el número de usuarios y posts presentes en el dataset. Se concluye:\n",
        "\n",
        "\n",
        "*   Hay muchos hipervínculos presentes en la data.\n",
        "*   Se asume que los hipervínculos no proveen ninguna información real acerca de la personalidad de los usuarios.\n",
        "*   Se prescinde de ellos para los análisis posteriores.\n",
        "Ha de tenerse en cuenta que los ejemplos dados vienen de los usuarios que han dejado comentarios; así, los modelos serán aplicados solamente a ellos (los que han dejado comentarios).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Con este *sesgo* nuestros modelos podrían fallar en clasificar las personalidades. Se comprobará más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qleggkomf-rE"
      },
      "source": [
        "def extract(posts, new_posts):\n",
        "    for post in posts[1].split(\"|||\"):\n",
        "        new_posts.append((posts[0], post))\n",
        "\n",
        "posts = []\n",
        "df.apply(lambda x: extract(x, posts), axis=1)\n",
        "print(\"Number of users\", len(df))\n",
        "print(\"Number of posts\", len(posts))\n",
        "print(\"5 posts from start are:\")\n",
        "posts[0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUJU51U-gID8"
      },
      "source": [
        "words = list(df[\"posts\"].apply(lambda x: x.split()))\n",
        "words = [x for y in words for x in y]\n",
        "Counter(words).most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psTI7nSzBJB8"
      },
      "source": [
        "**WORDCLOUD**\n",
        "**debería poner esto en parte de PLN??????**\n",
        "WordCloud es una técnica que permite mostrar las palabras que son más frecuentes en un texto dado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBAVuNqcBaCW"
      },
      "source": [
        "Se observa una perspectiva general del las palabras más frecuentes, en todo el dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF4qmLHegrLh"
      },
      "source": [
        "#Plotting the most common words with WordCloud.\n",
        "wc = wordcloud.WordCloud(width=1200, height=500, \n",
        "                         collocations=False, background_color=\"white\", \n",
        "                         colormap=\"tab20b\").generate(\" \".join(words))\n",
        "\n",
        "# collocations to False  is set to ensure that the word cloud doesn't appear as if it contains any duplicate words\n",
        "plt.figure(figsize=(25,10))\n",
        "# generate word cloud, interpolation \n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "_ = plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxEjLobkBvIq"
      },
      "source": [
        "Se muestran las palabras más comunes por cada tipo de personalidad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whFyAclKBiYk"
      },
      "source": [
        "fig, ax = plt.subplots(len(df['type'].unique()), sharex=True, figsize=(15,len(df['type'].unique())))\n",
        "k = 0\n",
        "for i in df['type'].unique():\n",
        "    df_4 = df[df['type'] == i]\n",
        "    wordcloud = WordCloud(max_words=1628,relative_scaling=1,normalize_plurals=False).generate(df_4['posts'].to_string())\n",
        "    plt.subplot(4,4,k+1)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(i)\n",
        "    ax[k].axis(\"off\")\n",
        "    k+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB8fAB9rCI4Q"
      },
      "source": [
        "Anteriormente se ha visto que la carga de los datos no es balanceada (no se tiene igual número de muestras para cada personalidad). Se muestran a continuación, para cada post, su representación por dimensiones. Se sabe que las dimensiones son excluyentes, así por ejemplo un post puede pertenecer o bien a N o bien a S. Esta pertenencia se construye a través de una representación binaria, donde 0 será la no pertenencia y 1 la pertenencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l3WTV3MgSPZ"
      },
      "source": [
        "def get_types(row):\n",
        "    t = row['type']\n",
        "\n",
        "    # Dimensiones: IE, NS, TF, JP\n",
        "    ## IE\n",
        "    I = 1 if t[0] == 'I' else 0 \n",
        "    ## NS\n",
        "    N = 1 if t[1] == 'N' else 0 \n",
        "    ## TF\n",
        "    T = 1 if t[2] == 'T' else 0 \n",
        "    ## JP\n",
        "    J = 1 if t[3] == 'J' else 0 \n",
        "\n",
        "    # Ejemplo: INFP (I = 1, N = 1, T = 0, J = 0)\n",
        "\n",
        "    return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) \n",
        "\n",
        "data_new = df.join(df.apply (lambda row: get_types (row),axis=1))\n",
        "data_new.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZjkvadsH3vm"
      },
      "source": [
        "A continuación se genera una cuenta del total de post para cada dimensión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lft4DEmYgc6f"
      },
      "source": [
        "print (\"Introversion (I) -  Extroversion (E):\\t\", data_new['IE'].value_counts()[1], \" / \", data_new['IE'].value_counts()[0])\n",
        "print (\"Intuition (N) – Sensing (S):\\t\\t\", data_new['NS'].value_counts()[1], \" / \", data_new['NS'].value_counts()[0])\n",
        "print (\"Thinking (T) – Feeling (F):\\t\\t\", data_new['TF'].value_counts()[1], \" / \", data_new['TF'].value_counts()[0])\n",
        "print (\"Judging (J) – Perceiving (P):\\t\\t\", data_new['JP'].value_counts()[1], \" / \", data_new['JP'].value_counts()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4O7vd4QID5D"
      },
      "source": [
        "A continuación se obtiene la representación gráfica de el conteo anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYkgj8e4ggLw"
      },
      "source": [
        "N = 4\n",
        "\n",
        "bott = (data_new['IE'].value_counts()[0], data_new['NS'].value_counts()[0], data_new['TF'].value_counts()[0], data_new['JP'].value_counts()[0])\n",
        "top = (data_new['IE'].value_counts()[1], data_new['NS'].value_counts()[1], data_new['TF'].value_counts()[1], data_new['JP'].value_counts()[1])\n",
        "\n",
        "ind = np.arange(N)    # the x locations for the groups\n",
        "width = 0.4      # the width of the bars: can also be len(x) sequence\n",
        "\n",
        "plt.figure(figsize=(4,3))\n",
        "p1 = plt.bar(ind, bott, width, color='cornflowerblue')\n",
        "p2 = plt.bar(ind, top, width, bottom=bott, color='aquamarine')\n",
        "\n",
        "plt.ylabel('Número de muestras')\n",
        "plt.title('Personalidades agrupadas en dimensiones')\n",
        "plt.xticks(ind, ('I/E',  'N/S', 'T/F', 'J/P',))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B69SdBntu15m"
      },
      "source": [
        "##**Preprocesamiento de datos.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G8OuANtISmE"
      },
      "source": [
        "**Estandarización de Datos**\n",
        "Vínculos y signos.\n",
        "Se procede a hacer un conteo de vínculos, signos de exclamación y signos de interrogación por cada tipo de personalidad.\n",
        "\n",
        "\n",
        "*   Se sustituye cada hipervínculo por la palabra LINK\n",
        "*   Se comprueba la existencia de signos de exclamación contiguos a direcciones web.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb_2LpzhhA9n"
      },
      "source": [
        "# Creación de una nueva columna para contener la información modificada de 'posts'\n",
        "df['posts_noLink'] = df['posts']\n",
        "\n",
        "import re\n",
        "for i in range(0, 8675):\n",
        "  df['posts_noLink'][i] = re.sub(r'http\\S+', 'LINK', df['posts'][i]);\n",
        "\n",
        "# Creación de las columnas 'links', 'qns' y 'exclams' para contener el total de enlaces, preguntas y exclamaciones, respectivamente\n",
        "df['links'] = df['posts'].apply(lambda x: x.count('http')); \n",
        "df['qns'] = df['posts_noLink'].apply(lambda x: x.count('?')); \n",
        "df['exclams'] = df['posts_noLink'].apply(lambda x: x.count('!')); \n",
        "df['words'] = df['posts'].apply(lambda x: len(x.split()))\n",
        "df['comments'] = df['words']\n",
        "for i in range (0, 8675):\n",
        "  df['comments'][i] = len(df['posts'][i].split('|||'))\n",
        "print(df.columns)\n",
        "df.head(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptnoKdXlptU2"
      },
      "source": [
        "**Medias aritméticas de vínculos, signos y palabras.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b63UCbzeOsQg"
      },
      "source": [
        "Con el método `groupBy` se grafica a continuación, la media aritmética de hipervínculos, signos de exclamación y signos de interrogación, por cada tipo de personalidad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdBOxWVmhdOZ"
      },
      "source": [
        "groupByColumns = df.groupby('type').agg({'links':'mean','qns':'mean','exclams':'mean'})\n",
        "\n",
        "plot = groupByColumns.plot(kind = 'bar', title = 'Media artimética de hipervínculos, interrogaciones y exclamaciones por tipo de personalidad')\n",
        "plot.set_xlabel(\"\")\n",
        "plot.set_ylabel(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPa6nV2XPQoY"
      },
      "source": [
        "A continuación, se observa la gráfica de la media artimética de palabras, por tipo de personalidad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlJCipnXiQA5"
      },
      "source": [
        "groupByWords = df.groupby('type').agg({'words':'mean'})\n",
        "print(\"Media de palabras {}\".format(np.mean(groupByWords)))\n",
        "plot2 = groupByWords.plot(kind = 'bar', title = 'Media de palabras por tipo de personalidad')\n",
        "plot2.set_xlabel(\"\")\n",
        "plot2.set_ylabel(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2o-m1duPej8"
      },
      "source": [
        "##**Procesamiento de lenguaje natural**\n",
        "Haciendo uso de la librería nltk, Natural Language Toolkit o en español, Kit de herramientas de lenguaje natural; se estudian las *stopwords* es decir, palabras de alta frecuencia como the, to, dentro del documento antes de procesarlo.\n",
        "Las palabras irrelevantes suelen tener poco contenido léxico y su presencia en un texto no lo distingue de otros textos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OgUO9afirCn"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english')); stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIYhBPVrRrnx"
      },
      "source": [
        "Con las *stopwords* encontradas anteriormente y usando `CountVectorizer`  se encuentran las frecuencias de las palabras de la columna posts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCF7_Eux5NGS"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(analyzer=\"word\", max_features=2000, strip_accents='ascii', stop_words=stop_words)\n",
        "# Frecuencia de cada una de las palabras de la columna 'posts'\n",
        "X_cnt = cv.fit_transform(df['posts'])\n",
        "# Diccionario de palabras\n",
        "cv.vocabulary_.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuC918CgbyFM"
      },
      "source": [
        "**Categorización de las personalidades.**\n",
        "Se procede a diferenciar cada usuario de acuerdo a uno de los cuatro elementos que forma cada tipo de personalidad; en su forma binaria (1 pertence, 0 no pertenece). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0av0zdPCbu-w"
      },
      "source": [
        "X = pd.DataFrame()\n",
        "for c in 'IENSTFJP':\n",
        "    X[c] = df['type'].apply(lambda x: 1 if c in x else 0)\n",
        "\n",
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpefSrvgSNPf"
      },
      "source": [
        "Se transforma lo anterior a un array.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQMzI1pO5SsI"
      },
      "source": [
        "X_cnt.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lncngDLiScBe"
      },
      "source": [
        "Se presenta un DataFrame en código binario para cada usuario, detallando la pertenencia o presencia de los elmentos identificados en hipervínculos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74h7pOIh5UgW"
      },
      "source": [
        "pd.DataFrame(X_cnt.toarray(), columns=['w_' + k for k in cv.vocabulary_.keys()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS5dS6ytTNsS"
      },
      "source": [
        "Al DataFrame **X** creado anteriormente, se le añaden las columnas anteriores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqCc0c1h9I0v"
      },
      "source": [
        "X = pd.concat([X, pd.DataFrame(X_cnt.toarray(), columns=['w_' + k for k in cv.vocabulary_.keys()])],\n",
        "              axis=1)\n",
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znNr5-ADTdnn"
      },
      "source": [
        "Se verifican las columnas que empiezan con la letra w que hace referencia a la palabra que se está tratando."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aJmoYv_-E-W"
      },
      "source": [
        "wcols = [col for col in X.columns if col.startswith('w_')]; wcols\n",
        "#X[wcols].mean() >= 0.5\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ly2DmVAU3mt"
      },
      "source": [
        "Frecuencia de cada palabra\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyZaV0O0-Pv8"
      },
      "source": [
        "# Selección de las palabras con frecuencia superior a 50% y trasposición entre filas y columnas\n",
        "# La trasposición es requerida porque la condición sobre las frecuencias devuelve True/False para cada palabra\n",
        "# en una misma  fila. Si no se realiza, no se puede aplicar dicha condición.\n",
        "X[wcols].T[X[wcols].mean() >= 0.5]\n",
        "\n",
        "# Trasposición de nuevo para volver a tener las palabras como columnas \n",
        "X[wcols].T[X[wcols].mean() >= 0.5].T\n",
        "\n",
        "# Creación de un nuevo DataFrame a partir del anterior que contenga las palabras más usadas.\n",
        "XX = X[wcols].T[X[wcols].mean() >= 0.5].T; XX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF5ELPEUVELc"
      },
      "source": [
        "Se calcula, a modo de ejemplo, la frecuencia media de las palabras citadas para el tipo E (extrovertido) y luego para el tipo I (Introvertido)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTof-wgK-bET"
      },
      "source": [
        "print('Frequencia de palabras para EXTROVERSION:')\n",
        "words_E = XX[X['E'] == 1].mean(); words_E"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIvYPdNq-qG5"
      },
      "source": [
        "print('Frequencia de palabras para INTROVERSION:')\n",
        "words_I = XX[X['I'] == 1].mean(); words_I"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4H5z9o0VSw3"
      },
      "source": [
        "Comparativa de palabras en Extroversión versus palabras en Introversión. Se diferencian 3 tipos de cocientes:\n",
        "\n",
        "\n",
        "*   Cociente que tiende a 0: significa que hay mucho mayor cantidad de palabras para el tipo Introversión.\n",
        "*   Cociente menor que 1: el número de palabras se asemeja para ambos tipos de personalidad.\n",
        "*   Cociente mayor que 1: el número  de palabras es mayor para el tipo Extroversión.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2RZbS7r-sFA"
      },
      "source": [
        "print('EXTROVERSION vs INTROVERSION Words')\n",
        "(words_E/words_I).rename(lambda x: x[2:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImKlO71LXCa4"
      },
      "source": [
        "Con la función `unique_words` se obtienen las palabras únicas más usadas por cada eje dicotómico de personalidad, a saber: \n",
        "\n",
        "\n",
        "*   I-E: Inroversión - Extroversión\n",
        "*  N-S: Intuition - Sensation\n",
        "*   T-F: Thinking-Feeling\n",
        "*   J-P: Judging-Perceiving\n",
        "\n",
        "Una vez contabilizadas, se grafican las comparaciones.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0TfrJ6C-2xB"
      },
      "source": [
        "# Extroversion and Introversion\n",
        "unique_words('E', 'I', 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqZBkB37_RUL"
      },
      "source": [
        "# Intuition and Sensing\n",
        "unique_words('N', 'S', 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki8kdySA_UIF"
      },
      "source": [
        "# Thinking and Feeling\n",
        "unique_words('T', 'F', 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w60cg_I_Wqm"
      },
      "source": [
        "# Judging and Perceiving\n",
        "unique_words('J', 'P', 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_cayV-WPKvR"
      },
      "source": [
        "**Análisis del coeficiente de correlación de Pearson con respecto al Procesamiento de Lenguaje Natural.**\n",
        "\n",
        "Después de la categorización de los datos, después del análisis de personalidades; para determinar de forma visual el análisis de relación entre las características de personlidades, se construye un mapa de calor para representar la colinealidad entre las característcas mencionadas.\n",
        "\n",
        "A través del coeficiente de Pearson, se verá la correlación entre los distintos descriptores que dan lugar a los 16 tipos de personalidad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVsu3TfbAgzP"
      },
      "source": [
        "# Desglose de cada personalidad en las características que contiene\n",
        "dfX = pd.DataFrame()\n",
        "for c in 'IENSTFJP':\n",
        "    dfX[c] = df['type'].apply(lambda x: 1 if c in x else 0)\n",
        "corr = dfX[['I','E','N','S','T','F','J','P']].corr(); corr\n",
        "cmap = plt.cm.RdBu\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.title('Matriz de correlación de Pearson', size=15)\n",
        "sns.heatmap(corr, cmap=cmap,  annot=True, linewidths=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWzuipMdY28e"
      },
      "source": [
        "Se continúa con el procesamiento de lenguaje natural.\n",
        "\n",
        "Se emplea el método TF_IDF para calcular la frecuencia de cada una de las palabras a partir del objeto TfidfTransformer, el cual generará un vector que será empleado para entrenar un modelo de procesamiento. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELAJiyLUafr0"
      },
      "source": [
        "Se preparan los datos que serán utilizados en los modelos de ML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnlfrKyKAuyH"
      },
      "source": [
        "# Recuperación de la información del objeto CountVectorizer\n",
        "feature_names = list(enumerate(cv.get_feature_names()))\n",
        "X_cnt.toarray()\n",
        "\n",
        "# Inicialización del objeto tf-idf para el cálculo de las frecuencias\n",
        "tfizer = TfidfTransformer()\n",
        "\n",
        "# Obtención de las frecuencias a partir del objeto CountVectorizer\n",
        "X_tfidf = tfizer.fit_transform(X_cnt).toarray()\n",
        "\n",
        "Y = X.filter(['I','E','N','S','T','F','J','P'], axis=1)\n",
        "\n",
        "# Añadido de las palabras y frecuencias al DataFrame con el desglose de personalidades\n",
        "testX = pd.concat([Y, pd.DataFrame(X_tfidf, columns=['w_' + k for k in cv.vocabulary_.keys()])],\n",
        "              axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR8Hy-HkaYef"
      },
      "source": [
        "Se observan las palabras específicas con relación a cada uno de los ejes dicotómicos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdRxPtOrBK8A"
      },
      "source": [
        "# Extroversion vs Introversion\n",
        "new_Words('E', 'I', 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tID5lV2oBNMg"
      },
      "source": [
        "# Thinking vs Feeling\n",
        "new_Words('T', 'F', 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjea780bBPn8"
      },
      "source": [
        "# Judging vs Perceiving\n",
        "new_Words('J', 'P', 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlKJchSMaJ5G"
      },
      "source": [
        "Se procede a un preprocesamiento donde se eliminan las publicaciones con menos de X palabras y posteriormente se crea un DataFrame con la información obtenida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVdgE7SEBYEE"
      },
      "source": [
        "#Preprocesamiento del texto introducido\n",
        "new_df = preprocess_text(df)\n",
        "\n",
        "#Eliminación de publicaciones con menos de X palabras\n",
        "min_words = 15\n",
        "print(\"Before : Numbero de publicaciones\", len(new_df)) \n",
        "new_df[\"no. of. words\"] = new_df[\"posts\"].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
        "new_df = new_df[new_df[\"no. of. words\"] >= min_words]\n",
        "\n",
        "print(\"Después : Numbero de publicaciones\", len(new_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-BGSXzvBosQ"
      },
      "source": [
        "new_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9YDlMBwbEBD"
      },
      "source": [
        "**LabelEncoder**: proporcionado por la biblioteca Sklearn que convierte los niveles de características categóricas (etiquetas) en forma numérica para convertirla en la forma legible por máquina. Codifica etiquetas con un valor entre 0 y n_classes-1 donde n es el número de etiquetas distintas. Si una etiqueta se repite, asigna el mismo valor al que se asignó anteriormente.\n",
        "\n",
        "Se muestra la conversión de la personalidad de MBTI (o objetivo o función Y) en forma numérica mediante la codificación de etiquetas con`LabelEncoder`  y se guarda en el DataFrame antes creado.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuqTGRSOBtDm"
      },
      "source": [
        "# Codificación del tipo de personalidad\n",
        "enc = LabelEncoder()\n",
        "new_df['type of encoding'] = enc.fit_transform(new_df['type'])\n",
        "\n",
        "target = new_df['type of encoding']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK1shayRBs9Y"
      },
      "source": [
        "new_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN0nMYXGbj2T"
      },
      "source": [
        "**CountVectorizer**  se usa para convertir una colección de documentos de texto en un vector de conteos de términos / tokens y construir un vocabulario de palabras conocidas, pero también para codificar nuevos documentos usando ese vocabulario. También permite el preprocesamiento de datos de texto antes de generar la representación vectorial.\n",
        "\n",
        "Se usa `stop_words = 'english'`  con CountVectorizer ya que esto sólo cuenta las ocurrencias de cada palabra en su vocabulario, palabras extremadamente comunes como 'the', 'and', etc. se convertirán en características muy importantes mientras agregan poco significado para el texto. Este es un paso importante en el procesamiento previo, ya que nuestro modelo a menudo se puede mejorar si no tiene en cuenta esas palabras.\n",
        "\n",
        "-  Se vectorizan las publicaciones para el modelo y se filtran palabras vacías.\n",
        "-  Conversión de publicaciones (o características X) en forma numérica usando vectorización de conteo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzz5_Q0wBz0e"
      },
      "source": [
        "# Vectorización por stop_words\n",
        "vect = CountVectorizer(stop_words='english') \n",
        "\n",
        "# Conversión de publicaciones\n",
        "train =  vect.fit_transform(new_df[\"posts\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er0dbwrVzUyl"
      },
      "source": [
        "####**hasta aquí PLN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy8po1ntcUk5"
      },
      "source": [
        "Se divide el conjunto de datos en entrenamiento y validación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32yl4FaXB018"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.4, stratify=target, random_state=42)\n",
        "print ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rnGorEoEGbM"
      },
      "source": [
        "accuracies = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8iRNkubzNbo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfFtUNgDhgwy"
      },
      "source": [
        "#**Implementación de algoritmos en Hard Computing**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlNj-xgMkZHu"
      },
      "source": [
        "####Definición de funciones \n",
        "A continuación definen una serie de funciones que será usadas para la implementación de los modelos.\n",
        "Se generan clases genéricas para visualización de resultados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqsGxFGLsn4M"
      },
      "source": [
        "class FuzzyKNN(BaseEstimator, ClassifierMixin):\n",
        "\tdef __init__(self, k=3, plot=False):\n",
        "\t\tself.k = k\n",
        "\t\tself.plot = plot\n",
        "\n",
        "\n",
        "\tdef fit(self, X, y=None):\n",
        "\t\tself._check_params(X,y)\n",
        "\t\tself.X = X\n",
        "\t\tself.y = y\n",
        "\n",
        "\t\tself.xdim = len(self.X[0])\n",
        "\t\tself.n = len(y)\n",
        "\n",
        "\t\tclasses = list(set(y))\n",
        "\t\tclasses.sort()\n",
        "\t\tself.classes = classes\n",
        "\n",
        "\t\tself.df = pd.DataFrame(self.X)\n",
        "\t\tself.df['y'] = self.y\n",
        "\n",
        "\t\tself.memberships = self._compute_memberships()\n",
        "\n",
        "\t\tself.df['membership'] = self.memberships\n",
        "\n",
        "\t\tself.fitted_ = True\n",
        "\t\treturn self\n",
        "\n",
        "\n",
        "\tdef predict(self, X):\n",
        "\t\tif self.fitted_ == None:\n",
        "\t\t\traise Exception('predict() called before fit()')\n",
        "\t\telse:\n",
        "\t\t\tm = 2\n",
        "\t\t\ty_pred = []\n",
        "\n",
        "\t\t\tfor x in X:\n",
        "\t\t\t\tneighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)\n",
        "\n",
        "\t\t\t\tvotes = {}\n",
        "\t\t\t\tfor c in self.classes:\n",
        "\t\t\t\t\tden = 0\n",
        "\t\t\t\t\tfor n in range(self.k):\n",
        "\t\t\t\t\t\tdist = np.linalg.norm(x - neighbors.iloc[n,0:self.xdim])\n",
        "\t\t\t\t\t\tden += 1 / (dist ** (2 / (m-1)))\n",
        "\n",
        "\t\t\t\t\tneighbors_votes = []\n",
        "\t\t\t\t\tfor n in range(self.k):\n",
        "\t\t\t\t\t\tdist = np.linalg.norm(x - neighbors.iloc[n,0:self.xdim])\n",
        "\t\t\t\t\t\tnum = (neighbors.iloc[n].membership[c]) / (dist ** (2 / (m-1)))\n",
        "\n",
        "\t\t\t\t\t\tvote = num/den\n",
        "\t\t\t\t\t\tneighbors_votes.append(vote)\n",
        "\t\t\t\t\tvotes[c] = np.sum(neighbors_votes)\n",
        "\n",
        "\t\t\t\tpred = max(votes.items(), key=operator.itemgetter(1))[0]\n",
        "\t\t\t\ty_pred.append((pred, votes))\n",
        "\n",
        "\t\t\treturn y_pred\n",
        "\n",
        "\n",
        "\tdef score(self, X, y):\n",
        "\t\tif self.fitted_ == None:\n",
        "\t\t\traise Exception('score() called before fit()')\n",
        "\t\telse:\n",
        "\t\t\tpredictions = self.predict(X)\n",
        "\t\t\ty_pred = [t[0] for t in predictions]\n",
        "\t\t\tconfidences = [t[1] for t in predictions]\n",
        "\n",
        "\t\t\treturn accuracy_score(y_pred=y_pred, y_true=y)\n",
        "\n",
        "\n",
        "\tdef _find_k_nearest_neighbors(self, df, x):\n",
        "\t\tX = df.iloc[:,0:self.xdim].values\n",
        "\n",
        "\t\tdf['distances'] = [np.linalg.norm(X[i] - x) for i in range(self.n)]\n",
        "\n",
        "\t\tdf.sort_values(by='distances', ascending=True, inplace=True)\n",
        "\t\tneighbors = df.iloc[0:self.k]\n",
        "\n",
        "\t\treturn neighbors\n",
        "\n",
        "\n",
        "\tdef _get_counts(self, neighbors):\n",
        "\t\tgroups = neighbors.groupby('y')\n",
        "\t\tcounts = {group[1]['y'].iloc[0]:group[1].count()[0] for group in groups}\n",
        "\n",
        "\t\treturn counts\n",
        "\n",
        "\n",
        "\tdef _compute_memberships(self):\n",
        "\t\tmemberships = []\n",
        "\t\tfor i,j in zip(self.X,self.y):\n",
        "\t\t\tx = i\n",
        "\t\t\ty = j\n",
        "\n",
        "\t\t\tneighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)\n",
        "\t\t\tcounts = self._get_counts(neighbors)\n",
        "\n",
        "\t\t\tmembership = dict()\n",
        "\t\t\tfor c in self.classes:\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\tuci = 0.49 * (counts[c] / self.k)\n",
        "\t\t\t\t\tif c == y:\n",
        "\t\t\t\t\t\tuci += 0.51\n",
        "\t\t\t\t\tmembership[c] = uci\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\tmembership[c] = 0\n",
        "\n",
        "\t\t\tmemberships.append(membership)\n",
        "\t\treturn memberships\n",
        "\n",
        "\n",
        "\tdef _check_params(self, X, y):\n",
        "\t\tif type(self.k) != int:\n",
        "\t\t\traise Exception('\"k\" should have type int')\n",
        "\t\tif self.k >= len(y):\n",
        "\t\t\traise Exception('\"k\" should be less than no of feature sets')\n",
        "\t\tif self.k % 2 == 0:\n",
        "\t\t\traise Exception('\"k\" should be odd')\n",
        "\n",
        "\t\tif type(self.plot) != bool:\n",
        "\t\t\traise Exception('\"plot\" should have type bool')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1KWx_eTSC2n"
      },
      "source": [
        "##Funciones relacionadas a los modelos\n",
        "def show_results(y_test, pred_y, txt):\n",
        "    print(txt, \" METRICS:\\n\")\n",
        "    conf_matrix = confusion_matrix(y_test, pred_y)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\")\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.ylabel('True class')\n",
        "    plt.xlabel('Predicted class')\n",
        "    plt.show()\n",
        "    print (classification_report(y_test, pred_y))\n",
        "\n",
        "# Representación de los datos de cada pasada con PYPLOT\n",
        "def draw_results(history):\n",
        "  ## Función de pérdida\n",
        "  plt.figure(1, figsize=(5,3))\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Función de pérdida')\n",
        "  plt.ylabel('Error')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Entrenamiento', 'Validación'], loc='upper left')\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  \n",
        "  ## Precisión (accuracy)\n",
        "  plt.figure(2, figsize=(5,3))\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('Precisión del modelo')\n",
        "  plt.ylabel('Precisión (accuracy)')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Entrenamiento', 'Validación'], loc='upper left')\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "#get_class_weights(): utilizado para la obtención de los pesos de cada una de las clases asociadas al entrenamiento en un modelo de redes neuronales.\n",
        "def get_class_weights(y, smooth_factor=0):\n",
        "    \"\"\"\n",
        "    Returns the weights for each class based on the frequencies of the samples\n",
        "    :param smooth_factor: factor that smooths extremely uneven weights\n",
        "    :param y: list of true labels (the labels must be hashable)\n",
        "    :return: dictionary with the weight for each class\n",
        "    \"\"\"\n",
        "    counter = Counter(y)\n",
        "\n",
        "    if smooth_factor > 0:\n",
        "        p = max(counter.values()) * smooth_factor\n",
        "        for k in counter.keys():\n",
        "            counter[k] += p\n",
        "\n",
        "    majority = max(counter.values())\n",
        "\n",
        "    return {cls: float(majority / count) for cls, count in counter.items()}\n",
        "\n",
        "\"\"\"\n",
        "# nameModel: nombre del modelo que se utiliza. Opciones 'XGB' para XGBClassifier \n",
        "    o 'NN' para un modelo de redes neuronales\n",
        "# model: la implementación del propio modelo. Opciones XGBClassifier o NN.\n",
        "# balanced: si se quiere utilizar un balanceo de los pesos de las clases \n",
        "    (menor prioridad en las mayoritarias)\n",
        "# oversamp: si se quieren generar datos sintéticos en las clases minoritarias \n",
        "    para evitar desequilibrio\n",
        "# undersamp: si se quieren eliminar aleatoriamente datos de la clase mayoritaria\n",
        "    para igualar el número total de muestras a las minoritarias\n",
        "# ep: número de pasos del proceso de entrenamiento (epochs). Por defecto con \n",
        "  valor 10\n",
        "# typeNN: tipo de red neuronal (1,2,3,4) que se quiere utilizar. \n",
        "  Creación y compilación antes del entrenamiento \n",
        "\"\"\"\n",
        "def train_process(model, nameModel, balanced = False, oversamp = False, undersamp = False, ep = 10,k=8):\n",
        "  features = 'IENSTFJP'\n",
        "  \n",
        "  # Estructura del modelo\n",
        "  if nameModel == 'NN':\n",
        "    model.summary()\n",
        "    features = 'INTJ'\n",
        "    \n",
        "  d = 0;\n",
        "  for i in features:\n",
        "      print(\"%s ...\" % (mbti[i]))\n",
        "      \n",
        "      # Entrenamiento de cada característica individualmente \n",
        "      Y_part = Y[i]\n",
        "\n",
        "      # Separación del dataset en entrenamiento (train) y validación (test)\n",
        "      seed = 7\n",
        "      test_size = 0.40\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X_tfidf, Y_part, test_size=test_size, random_state=seed)\n",
        "\n",
        "      #Se definen los conjuntos de validación y test desde el conjunto de test. \n",
        "      #Con relación a los datos originales, train es el 60%, test es el 25% y validation es el 15% \n",
        "      X_test,X_validation,y_test, y_validation = train_test_split(X_test, y_test, test_size=0.375, random_state=seed)\n",
        "      print(\"y_train: \", Counter(y_train)) # y_train.value_counts()\n",
        "\n",
        "      # Situación oversampling : VER LA DESCRIPCIÓN DE NOEMI UN POCOMÁS ARRIBA, LÍNEA 56 se generan muestras fake\n",
        "      # estructura común en los dos modelos\n",
        "      if oversamp:\n",
        "        from imblearn.over_sampling import SVMSMOTE\n",
        "        sm = SVMSMOTE(random_state=42)\n",
        "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)        \n",
        "        print (\"Distribution before resampling {}\".format(Counter(y_train)))\n",
        "        print (\"Distribution labels after resampling {}\".format(Counter(y_train_res)))\n",
        "        X_train = X_train_res;\n",
        "        y_train = y_train_res;\n",
        "      \n",
        "      # Situación undersampling, estructura común en los dos modelos\n",
        "      elif undersamp:\n",
        "        from imblearn.under_sampling import ClusterCentroids\n",
        "        cc = ClusterCentroids(random_state=0)\n",
        "        X_resampled, y_resampled = cc.fit_resample(X_train, y_train)\n",
        "        print(\"Undersampling: \", sorted(Counter(y_resampled).items()))\n",
        "        X_train = X_resampled;\n",
        "        y_train = y_resampled;\n",
        "\n",
        "      ## MODELO XGBCLASSIFIER\n",
        "      if nameModel == 'XGB':\n",
        "        if balanced:  #sE BALANCEAN los pesos con la función weight_balance\n",
        "          model.fit(X_train, y_train, sample_weight=compute_sample_weight(\"balanced\", y_train),verbose=2)\n",
        "        else:\n",
        "          model.fit(X_train, y_train,verbose=2) # aqui son pesos desbalancedos, generados por defecto para el modelo\n",
        "        \n",
        "        # PRECISIÓN ENTRENAMIENTO (Train Accuracy)\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        pred_train = [round(value) for value in y_pred_train]\n",
        "        accuracy_train = accuracy_score(y_train, pred_train)\n",
        "        print(\"* %s Train Accuracy: %.2f%%\" % (mbti[i], accuracy_train * 100.0))\n",
        "\n",
        "        # PRECISIÓN TEST (Test Accuracy)\n",
        "        # Predicciones a partir de los datos de prueba\n",
        "        y_pred = model.predict(X_test)\n",
        "        predictions = [round(value) for value in y_pred]\n",
        "        \n",
        "        #PRECISIÓN DE VALIDACION (dataset no entrenado)..será siempre el más bajo en cuanto a accuracy\n",
        "        #NOtar que los datos son totalmente nuevos para el modelo\n",
        "        y_pred_val = model.predict(X_validation)\n",
        "        predictions_validation = [round(value) for value in y_pred_val]\n",
        "\n",
        "        # Evaluación de las predicciones, cálculo de la precisión en validación\n",
        "        accuracy = accuracy_score(y_validation, predictions_validation)\n",
        "        print(\"* %s Test Accuracy: %.2f%%\" % (mbti[i], accuracy * 100.0))\n",
        "\n",
        "        show_results(y_train, pred_train, 'TRAIN')\n",
        "        show_results(y_test, predictions, 'TEST')  \n",
        "        show_results(y_validation, predictions_validation, 'VALIDATION') \n",
        "      \n",
        "      ## MODELO NN - NEURAL NETWORKS\n",
        "      elif nameModel == 'NN':\n",
        "\n",
        "        if balanced:\n",
        "          #class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "          weights = get_class_weights(y_train)\n",
        "          print(\"Class_weights:\", weights)\n",
        "          history = model.fit(X_train, y_train, epochs=ep, batch_size=len(X_train), validation_data=(X_test, y_test), \n",
        "                              class_weight=weights)\n",
        "        else:\n",
        "          history = model.fit(X_train, y_train, epochs=ep, batch_size=len(X_train), validation_data=(X_test, y_test))\n",
        "\n",
        "        d = d + 1\n",
        "        \n",
        "        # Evaluación del modelo (Accuracy) \n",
        "        print(\"Training\")\n",
        "        scores_train = model.evaluate(X_train, y_train, verbose=1) \n",
        "        print(\"\\nAccuracy TRAIN: %.2f%%\" % (scores_train[1]*100))\n",
        "        print(\"Testing\")\n",
        "        scores = model.evaluate(X_test, y_test, verbose=1) \n",
        "        print(\"Accuracy TEST: %.2f%%\" % (scores[1]*100))\n",
        "        \n",
        "        print(history)\n",
        "        # Representación de resultados\n",
        "        draw_results(history)\n",
        "      \n",
        "        y_pred_train = model.predict(X_train)\n",
        "        pred_train = [np.round(value) for value in y_pred_train]\n",
        "        show_results(y_train, pred_train, 'TRAIN')\n",
        "        y_pred = model.predict(X_test)\n",
        "        predictions = [np.round(value) for value in y_pred]\n",
        "        show_results(y_test, predictions, 'TEST')\n",
        "        y_pred = model.predict(X_validation)\n",
        "        predictions = [np.round(value) for value in y_pred]\n",
        "        show_results(y_validation, predictions, 'VALIDATION')\n",
        "        \n",
        "      \n",
        "      elif nameModel == \"KNN\":\n",
        "        for i in range(1,k):\n",
        "          knn = KNeighborsClassifier(i)\n",
        "          knn.fit(X_train, y_train)\n",
        "          print(\"KNN con {} clases\".format(i),end=\"\\n\")\n",
        "          print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
        "              .format(knn.score(X_train, y_train)))\n",
        "          print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
        "              .format(knn.score(X_test, y_test)))\n",
        "          print(\"Se inicia predicción\",end=\"\\n\")\n",
        "          pred = knn.predict(X_validation)\n",
        "          print(confusion_matrix(y_validation, pred))\n",
        "          print(classification_report(y_validation, pred))\n",
        "      \n",
        "      elif nameModel == \"fKNN\":\n",
        "        custModel = FuzzyKNN()        \n",
        "        custModel.fit(X_train, y_train)\n",
        "        print(cross_val_score(cv=5, estimator=custModel, X=X_test, y=y_test))\n",
        "          \n",
        "      print(\"--------------------------------------\")\n",
        "\n",
        "      del X_train, X_test, y_train, y_test, X_validation, y_validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0kxDU0Ckm5X"
      },
      "source": [
        "###**Modelo XGBOOST**\n",
        "\n",
        "Tengo que explicar lo de balanceado y desbalanceado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_rao_fMX0UB"
      },
      "source": [
        "#Se define el modelo\n",
        "model = XGBClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohY66PQXkvM7"
      },
      "source": [
        "Para datos desbalanceados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JnKTrSbktnb"
      },
      "source": [
        "train_process(model, 'XGB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVpyKdAwky8U"
      },
      "source": [
        "Para datos balanceados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBQGAVf9kz7C"
      },
      "source": [
        "train_process(model, 'XGB', balanced=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW_e4eLpYBp5"
      },
      "source": [
        "Para datos balanceados, con oversampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOHqdfk5k2Ve"
      },
      "source": [
        "train_process(model, 'XGB', balanced=False, oversamp=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYQU-KIkUoHT"
      },
      "source": [
        "A continuación se implementa  `XG boost Classifier` y se evalúa con la métrica `accuracy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joImsNSWU1gh"
      },
      "source": [
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train,y_train)\n",
        "\n",
        "Y_pred = xgb.predict(X_test)\n",
        "predictions = [round(value) for value in Y_pred]\n",
        "\n",
        "# Se evalúan las predicciones\n",
        "accuracy = accuracy_score(y_test, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR5fKVyOU4Ca"
      },
      "source": [
        "accuracies['XG Boost'] = accuracy* 100.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6BzNhonVBSm"
      },
      "source": [
        "****comprar resutados, falta..... **texto en negrita**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVXa9lhLU4zS"
      },
      "source": [
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IjJn-jDk4P4"
      },
      "source": [
        "###**REDES NEURONALES**\n",
        "Para los siguientes modelos se generan complicaciones determinadas  partir de la observación, así como de los resultados obtenidos por cada modelo.\n",
        "Se sigue un patrón de diseño de redes tipo cascada, donde para cada modelo nuevo de la red, se genran complicaciones basadas en el modelo inmediato anterior.\n",
        "Para todos los modelos subsecuentes se considera como métrica de bondad *accuracy*.\n",
        "Se genera una clase para el balanceo de los pesos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9T8J8rG2UUR"
      },
      "source": [
        "#####Definición de funciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7eGBsDSk7iz"
      },
      "source": [
        "import tensorflow.keras as keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5AVaVyqk-Uc"
      },
      "source": [
        "A continuación se definen los distintos optimizadpores que se serán utilizados en los modelos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzoW2x3gk9q-"
      },
      "source": [
        "lr = 0.01 # learning_rate (tasa de aprendizaje)\n",
        "\n",
        "## Optimizador SGD\n",
        "sgd = keras.optimizers.SGD(lr=lr)\n",
        "\n",
        "## Optimizador ADAGRAD\n",
        "adg = keras.optimizers.Adagrad(lr=lr, epsilon=None, decay=0.0)\n",
        "\n",
        "## Optimizador RMSprop\n",
        "rms = keras.optimizers.RMSprop(lr=lr, epsilon=None, decay=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkHvOZl7lCqU"
      },
      "source": [
        "#### **Modelo de Red Neuronal Número 1**\n",
        "Se crea una Red Neuronal Recurrente con las siguientes características:\n",
        "\n",
        "\n",
        "*   Una capa de entrada del tipo `Flatten` con  `Shape` de 2000\n",
        "*   Una capa intermedia con una densidad de 32 neuronas y función de activación ReLU\n",
        "*   Una capa de salida con densidad de 1 con función de activación *Sigmoide*\n",
        "*   El optimizador es SGD \n",
        "*   La función de pérdida es Entropía Cruzada Binaria\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Qbpz2_lP8D"
      },
      "source": [
        "def create_compileNN():\n",
        "  model1 = keras.models.Sequential()\n",
        "  model1.add(keras.layers.Flatten(input_shape=(2000,))) # capa de entrada\n",
        "  model1.add(keras.layers.Dense(32, activation='relu')) # capa oculta\n",
        "  model1.add(keras.layers.Dense(1, activation='sigmoid')) # capa de salida\n",
        "  \n",
        "  # Compilación del modelo\n",
        "  model1.compile(optimizer=sgd, \n",
        "                loss='binary_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model1\n",
        "\n",
        "model1 = create_compileNN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoYn1XkQlVAl"
      },
      "source": [
        "#####Primera mejora de hiperparámetros.\n",
        "\n",
        "Para el primer proceso de entrenamiento se generan iteraciones con pasos de 2 unidades en el rango 5-19 que determinan el número de epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB1FZM97lWxj"
      },
      "source": [
        "for i in range (5, 20, 2):\n",
        "  print('NUMBER OF EPOCHS: ', i)\n",
        "  train_process(model1, 'NN', ep=i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT0oDWJglZL9"
      },
      "source": [
        "#####Segunda mejora de hiperparámetros.\n",
        "\n",
        "Para este segundo proceso de entrenamiento se generan iteraciones con pasos de 6 unidades (para tomar el valor medio)en el rango 7-19 que determinan el número de epochs.\n",
        "Dado que en en la mejora anterior se obtuvo que los mejores valores están entre 7 y 19, se genera la validación del  modelo a partir de dicho rango."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x30UIrnilbWi"
      },
      "source": [
        "for i in range (7, 20, 6):\n",
        "  print('NUMBER OF EPOCHS: ', i)\n",
        "  train_process(model1, 'NN', ep=i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrR2_SbDlexo"
      },
      "source": [
        "#### **Modelo de Red Neuronal Número 2**\n",
        "\n",
        "Se genera una segunda Red Neuronal de tipo Recurrente con las siguientes características:\n",
        "\n",
        "\n",
        "\n",
        "*   Capa de entrada del tipo `Flatten` con `Shape` de 2000 (Igual que en la red anterior)\n",
        "* Se genran n capas intermedias considerando densidades descritas en el arreglo *neuron*  con función de activación ReLU \n",
        "*   Capa de salida de tipo `Dense` con densidad de 1 y función de activación *Sigmoide*.\n",
        "*   El optimizador es SGD \n",
        "*   La función de pérdida es Entropía Cruzada Binaria\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6JDFy9sll2G"
      },
      "source": [
        "def create_compileNN(lyrs=1, neuron=[32]):\n",
        "  model2 = keras.models.Sequential()\n",
        "  model2.add(keras.layers.Flatten(input_shape=(2000,))) # capa de entrada\n",
        "  for i in range (0, lyrs):\n",
        "    model2.add(keras.layers.Dense(neuron[i], activation='relu')) # capa oculta i\n",
        "  model2.add(keras.layers.Dense(1, activation='sigmoid')) # capa de salida\n",
        "\n",
        "  model2.compile(optimizer=sgd, \n",
        "                loss='binary_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "  return model2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N10En1tylo6h"
      },
      "source": [
        "Se generan 5 variaciones del modelo original donde se consideran 19 epochs (obtenidas del modelo anterior).\n",
        "\n",
        "Se generan *complicaciones* al modelo:\n",
        "\n",
        "1. Consta de 2 capas intermedias con densidad de 32 neuronas cada una.\n",
        "2. Consta de 2 capas intemedias con 64 y 32 neuronas, esta red emula una red deconvolucional que por definición empieza por una densidad mayor a la con que se termina, considerando en todo momento las potencias de 2.\n",
        "3. Consta de 2 capas intermedias con 64 neuronas cada una.\n",
        "4. Consta de 3 capas intermedias con 32, 64 y 32 neuronas cada una. Esta capa en concreto emula una red convolucional-deconvolucional. \n",
        "5. Consta de 3 capas intermedia con 64,64 y 32 neuronas. Esta capa estaría emulando un red deconvolucional con una complicación específica; hablamos aquí de capas gemelas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxA3ki1ilqvX"
      },
      "source": [
        "epochs = 19\n",
        "\n",
        "# CAPAS OCULTAS: 2 de 32\n",
        "layers = 2; neuron = [32,32]\n",
        "model2 = create_compileNN(layers, neuron)\n",
        "train_process(model2, 'NN', ep=epochs)\n",
        "\n",
        "# CAPAS OCULTAS: 2 de 64 y 32\n",
        "layers = 2; neuron = [64,32]\n",
        "model2 = create_compileNN(layers, neuron)\n",
        "train_process(model2, 'NN', ep=epochs)\n",
        "\n",
        "# CAPAS OCULTAS: 2 de 64\n",
        "layers = 2; neuron = [64,64]\n",
        "model2 = create_compileNN(layers, neuron)\n",
        "train_process(model2, 'NN', ep=epochs)\n",
        "\n",
        "# CAPAS OCULTAS: 3 de 32, 64 y 32\n",
        "layers = 3; neuron = [32,64,32]\n",
        "model2 = create_compileNN(layers, neuron)\n",
        "train_process(model2, 'NN', ep=epochs)\n",
        "\n",
        "# CAPAS OCULTAS: 3 de 64, 64 y 32\n",
        "layers = 3; neuron = [64,64,32]\n",
        "model2 = create_compileNN(layers, neuron)\n",
        "train_process(model2, 'NN', ep=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh2CqoSGltLP"
      },
      "source": [
        "####**Modelo de Red Neuronal Número 3**\n",
        "Se genera una Red Neuronal Recurrente con las siguientes características:\n",
        "* Capa de entrada del tipo Flatten con Shape de 2000.\n",
        "* Las 3 capas ocultas constan con densidad 64,64,32 y función de activación ReLU.\n",
        "*   Capa de salida de tipo `Dense` con densidad de 1 y función de activación *Sigmoide*.\n",
        "*   Se generan 9 submodelos que se pueden observar en la segunda celda de este apartado:\n",
        "\n",
        "  \n",
        "1. Optimizador AdamGrad con 7 epochs.\n",
        "2. Optimizador AdamGrad con 13 epochs.\n",
        "3. Optimizador AdamGrad con 19 epochs.\n",
        "4. Optimizador  RMSprop con 7 epochs.\n",
        "5. Optimizador  RMSprop con 13 epochs.\n",
        "6. Optimizador  RMSprop con 19 epochs.\n",
        "\n",
        "*   La función de pérdida es Entropía Cruzada Binaria.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TZMmj63l08T"
      },
      "source": [
        "# Modelo con dos capas ocultas, una con 64 y otra con 32 neuronas\n",
        "def create_compileNN(optimizer):\n",
        "  model3 = keras.models.Sequential()\n",
        "  model3.add(keras.layers.Flatten(input_shape=(2000,))) # capa de entrada\n",
        "  model3.add(keras.layers.Dense(64, activation='relu')) # capa oculta 1\n",
        "  model3.add(keras.layers.Dense(64, activation='relu')) # capa oculta 2\n",
        "  model3.add(keras.layers.Dense(32, activation='relu')) # capa oculta 3\n",
        "  model3.add(keras.layers.Dense(1, activation='sigmoid')) # capa de salida\n",
        "\n",
        "  model3.compile(optimizer=optimizer, \n",
        "                loss='binary_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "  print('Modelo creado y compilado, ', optimizer)\n",
        "  return model3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkTyV5rFl2hK"
      },
      "source": [
        "epochs = [7, 13, 19]\n",
        "opts = [adg, rms]\n",
        "for i in opts:\n",
        "  for j in epochs:\n",
        "    model3 = create_compileNN(i)\n",
        "    train_process(model3, 'NN', ep=j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_yrx5nSl4JL"
      },
      "source": [
        "#### **Modelo de Red Neuronal Número 4**\n",
        "Se genera una Red Neuronal Recurrente con las siguientes características:\n",
        "\n",
        "* Capa de entrada del tipo Flatten con Shape de 2000.\n",
        "* Consta de una capa tipo Dense con 64 neuronas y función de activación ReLU.\n",
        "* Una capa Dropout con rango de pérdida entre 10% y 30%, dicha capa es alimentada a partir de un ciclo iterativo considerando los valores del rango.\n",
        "* Dos capas ocultas de tipo Dense con 64 y 32 neuronas. Función de activación ReLU.\n",
        "* Capa de salida del tipo Dense, con 1 neurona y función de activación Sigmoide.\n",
        "* El optimizador es RMSProp\n",
        "* La función de pérdida es Entropía Cruzada Binaria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FRp8jkvo093"
      },
      "source": [
        "#####Variación Modelo 4a:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUgUbmUZmA0C"
      },
      "source": [
        "# UNA CAPA DROPOUT AL INICIO 10%, 20%, 30%; 19 epochs\n",
        "percen = [0.1, 0.2, 0.3]\n",
        "i=0\n",
        "\n",
        "def create_compileNN():\n",
        "  model4 = keras.models.Sequential()\n",
        "  model4.add(keras.layers.Flatten(input_shape=(2000,))) # capa de entrada\n",
        "  model4.add(keras.layers.Dense(64, activation='relu')) # capa oculta 1\n",
        "  model4.add(keras.layers.Dropout(percen[i])) # capa Dropout\n",
        "  model4.add(keras.layers.Dense(64, activation='relu')) # capa oculta 2\n",
        "  model4.add(keras.layers.Dense(32, activation='relu')) # capa oculta 3\n",
        "  model4.add(keras.layers.Dense(1, activation='sigmoid')) # capa de salida\n",
        "\n",
        "  model4.compile(optimizer=rms, \n",
        "                loss='binary_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "  print('Modelo creado: Dropout ', percen[i])\n",
        "  return model4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbBzzOIpmDYo"
      },
      "source": [
        "# UNA CAPA DROPOUT AL INICIO 10%, 20%, 30%; 19 epochs\n",
        "for i in range(0, len(percen)):\n",
        "  model4 = create_compileNN()\n",
        "  train_process(model4, 'NN', ep = 17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBaFuiqUourU"
      },
      "source": [
        "#####Variación Modelo 4b:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMQDR7NOmGZ6"
      },
      "source": [
        "# DOS CAPAS DROPOUT DE 0.1\n",
        "def create_compileNN():\n",
        "  model4 = keras.models.Sequential()\n",
        "  model4.add(keras.layers.Flatten(input_shape=(2000,))) # capa de entrada\n",
        "  model4.add(keras.layers.Dense(64, activation='relu')) # capa oculta 1\n",
        "  model4.add(keras.layers.Dropout(0.1)) # capa Dropout\n",
        "  model4.add(keras.layers.Dense(64, activation='relu')) # capa oculta 2\n",
        "  model4.add(keras.layers.Dropout(0.1)) # capa Dropout\n",
        "  model4.add(keras.layers.Dense(32, activation='relu')) # capa oculta 3\n",
        "  model4.add(keras.layers.Dense(1, activation='sigmoid')) # capa de salida\n",
        "\n",
        "  model4.compile(optimizer=rms, \n",
        "                loss='binary_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "  return model4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dm8GkylmIO9"
      },
      "source": [
        "# DOS CAPAS DROPOUT DE 0.1; 19 epochs\n",
        "model4 = create_compileNN()\n",
        "train_process(model4, 'NN', ep = 19)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRw2xTf-U184"
      },
      "source": [
        "#**Implementación de algoritmos en Soft Computing**\n",
        "se abre la posibilidad de generar una mejora en los resultados obtenidos, con los modelos KNN y fKNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRHmuUXQcpQm"
      },
      "source": [
        "### **KNeighborsClassifier**\n",
        "- Considerando las características de los modelos KNN se tomó la decisión de generar un modelo multiclase para la generación de los resultados en las predicciones de las características.\n",
        "- Se evalúa con accuracy\n",
        "- Se buscan los mejores parámetros con `scoreList`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdEOp5ymRYq4"
      },
      "source": [
        "Y_part = Y[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA4LWoA3RYX4"
      },
      "source": [
        "Y_part = Y_part.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-fRJjvoG-Vx"
      },
      "source": [
        "seed = 7\n",
        "test_size = 0.40\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, Y_part, test_size=test_size, random_state=seed)\n",
        "\n",
        "#Se definen los conjuntos de validación y test desde el conjunto de test. \n",
        "#Con relación a los datos originales, train es el 60%, test es el 25% y validation es el 15% \n",
        "X_test,X_validation,y_test, y_validation = train_test_split(X_test, y_test, test_size=0.375, random_state=seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdPTUgjVHDvf"
      },
      "source": [
        "#### KNN 8 clases\n",
        "Se define un modelo con 8 clases y se entrena."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snr3jnm5HDfD"
      },
      "source": [
        "knn = KNeighborsClassifier(8)\n",
        "knn.fit(X_train, y_train)\n",
        "print(\"KNN con {} clases\".format(8),end=\"\\n\")\n",
        "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
        "    .format(knn.score(X_train, y_train)))\n",
        "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
        "    .format(knn.score(X_test, y_test)))\n",
        "print(\"Se inicia predicción\",end=\"\\n\")\n",
        "pred = knn.predict(X_validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMw8MHkpG-dX"
      },
      "source": [
        "print(classification_report(y_validation, pred)) #Se muestra el reporte de todas las clases\n",
        "#enumaradas de 0 a 7, sin necesidad de usar lógica difusa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0G0Hk92HSt5"
      },
      "source": [
        "lbls = [i for i in Y.columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAjZ6-P4RX3A"
      },
      "source": [
        "Se define la matriz de confusión del algoritmo antes entrenado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn0N-lDfHXvW"
      },
      "source": [
        "cnfsm = multilabel_confusion_matrix(y_validation, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YcsUpYnHaP2"
      },
      "source": [
        "Se crea una función que imprime los valores obtenidos tras la aplicación del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UypO6F2nHcA0"
      },
      "source": [
        "import seaborn as sns\n",
        "vis_arr = np.asarray(cnfsm)\n",
        "\n",
        "def print_confusion_matrix(confusion_matrix, axes, class_label, class_names, fontsize=14):\n",
        "\n",
        "    df_cm = pd.DataFrame(\n",
        "        confusion_matrix, index=class_names, columns=class_names,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes)\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
        "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
        "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
        "    axes.set_ylabel('True label')\n",
        "    axes.set_xlabel('Predicted label')\n",
        "    axes.set_title(\"Confusion Matrix for the class - \" + class_label)\n",
        "\n",
        "fig, ax = plt.subplots(4, 2, figsize=(12, 9))\n",
        "for axes, cnfsm, label in zip(ax.flatten(), vis_arr, lbls):\n",
        "    print_confusion_matrix(cnfsm, axes, label, [\"N\", \"Y\"])\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show() #se generan las gráficas de sitorsión para cada una dde las clases de personlidad\n",
        "#considerando valores predichos versus reales, con un eje de predicción y un eje de valor\n",
        "#tiene 2 subejes binarios al valor de la predicción"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1JRGyq6HelK"
      },
      "source": [
        "#### KNN 16 clases\n",
        "Se define un modelo con 8 clases y se entrena."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnkCUA_bHqi0"
      },
      "source": [
        "#Se definen las clases\n",
        "lbls\n",
        "y_train_text=[]\n",
        "y_test_text=[]\n",
        "y_validation_text=[]\n",
        "\n",
        "for i in y_validation: \n",
        "  temp=np.where(i==1)\n",
        "  y_validation_text.append(\"\".join(lbls[x] for x in temp[0]))\n",
        "\n",
        "for i in y_train: \n",
        "  temp=np.where(i==1)\n",
        "  y_train_text.append(\"\".join(lbls[x] for x in temp[0]))\n",
        "\n",
        "for i in y_test: \n",
        "  temp=np.where(i==1)\n",
        "  y_test_text.append(\"\".join(lbls[x] for x in temp[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfTRE5CZHsvl"
      },
      "source": [
        "#Se construye el modelo\n",
        "knn = KNeighborsClassifier(16)\n",
        "knn.fit(X_train, y_train_text)\n",
        "print(\"KNN con {} clases\".format(16),end=\"\\n\")\n",
        "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
        "    .format(knn.score(X_train, y_train_text)))\n",
        "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
        "    .format(knn.score(X_test, y_test_text)))\n",
        "print(\"Se inicia predicción\",end=\"\\n\")\n",
        "pred = knn.predict(X_validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTReUIIbHudn"
      },
      "source": [
        "#Se corroborán las predicciones\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TZAlac0Hybi"
      },
      "source": [
        "print(classification_report(y_validation_text, pred)) #Se muestra el reporte de todas las clases\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIVwKtHbH2g8"
      },
      "source": [
        "cnfsm = multilabel_confusion_matrix(y_validation_text, pred)\n",
        "vis_arr = np.asarray(cnfsm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWYMmAR5H5H2"
      },
      "source": [
        "fig, ax = plt.subplots(8, 2, figsize=(12, 24))\n",
        "\n",
        "def print_confusion_matrix(confusion_matrix, axes, class_label, class_names, fontsize=14):\n",
        "\n",
        "    df_cm = pd.DataFrame(\n",
        "        confusion_matrix, index=class_names, columns=class_names,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes)\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
        "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
        "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
        "    axes.set_ylabel('True label')\n",
        "    axes.set_xlabel('Predicted label')\n",
        "    axes.set_title(\"Confusion Matrix for the class - \" + class_label)\n",
        "\n",
        "\n",
        "for axes, cnfsm, label in zip(ax.flatten(), vis_arr, set(y_train_text)):\n",
        "    print_confusion_matrix(cnfsm, axes, label, [\"N\", \"Y\"])\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show() #se generan las gráficas de sitorsión para cada una dde las clases de personlidad\n",
        "#considerando valores predichos versus reales, con un eje de predicción y un eje de valor\n",
        "#tiene 2 subejes binarios al valor de la predicción"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CI1aqweH8Qo"
      },
      "source": [
        "### **KNeighborsClassifier Difuso**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI1rdDITIDkK"
      },
      "source": [
        " from sklearn.base import BaseEstimator, ClassifierMixin\n",
        " from sklearn.model_selection import cross_val_score\n",
        " import operator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf3nOsihIFZs"
      },
      "source": [
        "train_process(None, nameModel='fKNN' )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}